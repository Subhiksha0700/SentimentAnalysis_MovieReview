{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcc5e5f-4a7b-4a6f-b6bf-a6ef5597ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from sentiment_read_subjectivity import readSubjectivity\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lexicon_path = \"/Users/subhiksha/Documents/NLP/subjclueslen1-HLTEMNLP05.tff\"\n",
    "data = []\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "     \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def processkaggle(dirPath,flag):\n",
    "    os.chdir(dirPath)\n",
    "    if flag=='train':\n",
    "        filepath = './train.tsv'\n",
    "    else:\n",
    "        filepath = './test.tsv'\n",
    "    with open(filepath, 'r') as f:   \n",
    "        phrasedata = []\n",
    "        for line in f:\n",
    "            if not line.startswith('Phrase'):\n",
    "                line = line.strip()\n",
    "                parts = line.split('\\t')           \n",
    "                if flag == 'train':\n",
    "                    phrasedata.append((parts[2], parts[3])) \n",
    "                else:\n",
    "                    phrasedata.append(parts[-1])\n",
    "\n",
    "    samples_per_class = 4000\n",
    "    balanced_data = []\n",
    "    labels = ['0', '1', '2', '3', '4']   \n",
    "    for label in labels:\n",
    "        class_data = [item for item in phrasedata if item[1] == label]\n",
    "        if len(class_data) >= samples_per_class:\n",
    "            class_sample = class_data[:samples_per_class]\n",
    "        else:\n",
    "            class_sample = class_data \n",
    "        balanced_data.extend(class_sample)\n",
    "        \n",
    "    random.shuffle(balanced_data)\n",
    "    \n",
    "    phraselist = balanced_data\n",
    "    # if flag=='train': \n",
    "    #     phraselist = phrasedata[:20000]\n",
    "    # if flag=='test':\n",
    "    #     phraselist = phrasedata[:10000]\n",
    "    \n",
    "    #nltk.download('stopwords')\n",
    "    nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "    morestopwords = ['could', 'would', 'might', 'must', 'need', 'sha', 'wo', 'y', \"'s\", \"'d\", \"'ll\", \"'t\", \"'m\", \"'re\", \"'ve\", \"n't\"]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    #pos_list, neutral_list, neg_list = readSubjectivity(lexicon_path)\n",
    "    \n",
    "    output_lines = []\n",
    "    for item in phraselist:\n",
    "        if flag == 'train':\n",
    "            phrase, label = item\n",
    "        else:\n",
    "            phrase = item\n",
    "        tokens = nltk.word_tokenize(phrase)\n",
    "        stopwords = set(nltkstopwords+morestopwords) \n",
    "        stopwords = [word for word in stopwords if word not in negationwords]\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "        filtered_tokens = [word for word in lemmatized_tokens if word.lower() not in stopwords and not all(char in punctuation for char in word)]\n",
    "        if flag == 'train':  \n",
    "            output_lines.append(','.join(filtered_tokens) + ',' + label)\n",
    "        else:  \n",
    "            output_lines.append(','.join(filtered_tokens))\n",
    "        \n",
    "    return output_lines\n",
    "\n",
    "  \n",
    "path= \"/Users/subhiksha/Documents/NLP/NLP project/FinalProjectData/kagglemoviereviews/corpus\"\n",
    "train_data = processkaggle(path,'train')\n",
    "#test_data = processkaggle(path,'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67175a7b-f0e5-4dfc-b82c-4bd478708243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14457\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  220576 lr:  0.000000 avg.loss:  1.899183 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.61655773 0.61934512 0.62142766 0.61518006 0.61649366]\n",
      "Average accuracy: 0.6178008458285275\n"
     ]
    }
   ],
   "source": [
    "# import fasttext\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Train a FastText model\n",
    "# model = fasttext.train_unsupervised('/Users/subhiksha/Documents/NLP/train_data.txt', model='skipgram')\n",
    "\n",
    "# # Function to convert document to mean of word vectors\n",
    "# def document_vector(doc):\n",
    "#     word_vectors = [model.get_word_vector(word) for word in doc]\n",
    "#     return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.get_dimension())\n",
    "\n",
    "# # Extract documents and labels\n",
    "\n",
    "# documents = [line.split(',')[:-1] for line in train_data]  \n",
    "# labels = [int(line.split(',')[-1]) for line in train_data]  \n",
    "\n",
    "# X = np.array([document_vector(doc) for doc in documents])\n",
    "# y = np.array(labels)\n",
    "\n",
    "# # classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "# classifier = RandomForestClassifier()\n",
    "\n",
    "# classifier.fit(X, y) \n",
    "# kf = KFold(n_splits=5, random_state=42, shuffle=True)  \n",
    "\n",
    "# scores = cross_val_score(classifier, X, y, cv=kf)\n",
    "\n",
    "# print(\"Cross-validation scores:\", scores)\n",
    "# print(\"Average accuracy:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b193f59f-6754-4bb6-88c4-6cb4327c5cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been saved to: /Users/subhiksha/Documents/NLP/test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "test_documents = [line for line in test_data]\n",
    "X_test = np.array([document_vector(doc.split(',')) for doc in test_documents])\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "output_csv_path = '/Users/subhiksha/Documents/NLP/test_predictions.csv'\n",
    "with open(output_csv_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Phrase', 'Prediction']) \n",
    "    for phrase, prediction in zip(test_documents, y_pred):\n",
    "        writer.writerow([phrase, prediction])\n",
    "\n",
    "print(\"Predictions have been saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f68c5f-085a-4ce0-85b9-91a174d6f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sldict = readSubjectivity(lexicon_path)\n",
    "processed_train_data = [(doc[:-2].split(','), doc[-1]) for doc in train_data]\n",
    "\n",
    "def document_features(document, word_features, negationwords, SL):\n",
    "    document_words = document\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "    features['positivecount'] = weakPos + (2 * strongPos)\n",
    "    features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "            \n",
    "    return features\n",
    "\n",
    "# all_words_list = [word for document in train_data for word in document]\n",
    "# new_all_words = nltk.FreqDist(all_words_list)\n",
    "# new_word_features = list(new_all_words)[:2000] \n",
    "\n",
    "\n",
    "# common_words_train = set(word for document in train_data for word in document[0])\n",
    "# train_featuresets = [(document_features(doc, new_word_features,negationwords,sldict), doc[-1]) for doc in train_data]\n",
    "\n",
    "\n",
    "# Build a list of all words in all documents\n",
    "all_words_list = [word for document, label in processed_train_data for word in document]\n",
    "all_words_freq = nltk.FreqDist(all_words_list)\n",
    "word_features = list(all_words_freq)[:2000]  # top 2000 words as features\n",
    "\n",
    "# Generate features for each document\n",
    "train_featuresets = [(document_features(doc, word_features, negationwords, sldict), label) for doc, label in processed_train_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c45dfd-84ec-4ba8-8dcb-2d3c6c79b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.632\n",
      "Fold 2: Accuracy = 0.66175\n",
      "Fold 3: Accuracy = 0.639\n",
      "Fold 4: Accuracy = 0.64625\n",
      "Fold 5: Accuracy = 0.65075\n",
      "Average Accuracy across all folds: 0.6459499999999999\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def cross_validation_PRF(num_folds, featuresets):\n",
    "    subset_size = len(featuresets) // num_folds\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i * subset_size:][:subset_size]\n",
    "        train_this_round = featuresets[:i * subset_size] + featuresets[(i + 1) * subset_size:]\n",
    "\n",
    "        # logistic_regression = SklearnClassifier(LogisticRegression(random_state=42, max_iter=300))\n",
    "        # logistic_regression.train(train_this_round)\n",
    "        # accuracy = nltk.classify.accuracy(logistic_regression, test_this_round)\n",
    "        \n",
    "        # classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # accuracy = nltk.classify.accuracy(classifier, test_this_round)\n",
    "\n",
    "        rf_classifier = SklearnClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        rf_classifier.train(train_this_round)\n",
    "        accuracy = nltk.classify.accuracy(rf_classifier, test_this_round)\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(f\"Fold {i+1}: Accuracy = {accuracy}\")\n",
    "\n",
    "    avg_accuracy = sum(accuracy_list) / num_folds\n",
    "    print(f\"Average Accuracy across all folds: {avg_accuracy}\")\n",
    "\n",
    "cross_validation_PRF(5,train_featuresets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00597660-3912-439c-96e2-d2c04ad3f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     words = document[0]\n",
    "#     features = {}\n",
    "    \n",
    "#     # # Include bigrams and trigrams        \n",
    "#     # bigrams = [\"_\".join(bigram) for bigram in ngrams(words, 2) if all(w in word_features for w in bigram)]\n",
    "#     # trigrams = [\"_\".join(trigram) for trigram in ngrams(words, 3) if all(w in word_features for w in trigram)]\n",
    "#     # for bigram in bigrams:\n",
    "#     #     features[f'contains_bigram({bigram})'] = True\n",
    "#     # for trigram in trigrams:\n",
    "#     #     features[f'contains_trigram({trigram})'] = True\n",
    "\n",
    "#     # for word in word_features:\n",
    "#     #     features['V_{}'.format(word)] = False\n",
    "#     #     features['V_NOT{}'.format(word)] = False\n",
    "#     # # go through document words in order\n",
    "#     # for i in range(0, len(words)):\n",
    "#     #     word = words[i]\n",
    "#     #     if ((i + 1) < len(words)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "#     #         i += 1\n",
    "#     #         features['V_NOT{}'.format(words[i])] = (words[i] in word_features)\n",
    "#     #     else:\n",
    "#     #         features['V_{}'.format(word)] = (word in word_features)\n",
    "\n",
    "\n",
    "# # tagged = nltk.pos_tag(words)\n",
    "#     # for word, tag in tagged:\n",
    "#     #     pos_feature = f'POS_{tag}'\n",
    "#     #     if pos_feature in features:\n",
    "#     #         features[pos_feature] += 1\n",
    "#     #     else:\n",
    "#     #         features[pos_feature] = 1\n",
    "\n",
    "#     # Using subjectivity lexicon for sentiment strength and polarity\n",
    "#     # for word in words:\n",
    "#     #     if word in SL:\n",
    "#     #         subj_info = sldict[word]\n",
    "#     #         features[f'subjectivity_strength({word})'] = subj_info[0]  # strength\n",
    "#     #         features[f'polarity({word})'] = subj_info[3]  # polarity\n",
    "             \n",
    "#     # Adding POS tags as features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
