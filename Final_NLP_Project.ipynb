{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aaa59cb-9b29-4807-bf66-11ef40c740ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from sentiment_read_subjectivity import readSubjectivity\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lexicon_path = \"/Users/subhiksha/Documents/NLP/subjclueslen1-HLTEMNLP05.tff\"\n",
    "data = []\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "     \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def processkaggle(dirPath,flag):\n",
    "    os.chdir(dirPath)\n",
    "    if flag=='train':\n",
    "        filepath = './train.tsv'\n",
    "    else:\n",
    "        filepath = './test.tsv'\n",
    "    with open(filepath, 'r') as f:   \n",
    "        phrasedata = []\n",
    "        for line in f:\n",
    "            if not line.startswith('Phrase'):\n",
    "                line = line.strip()\n",
    "                parts = line.split('\\t')           \n",
    "                if flag == 'train':\n",
    "                    phrasedata.append((parts[2], parts[3])) \n",
    "                else:\n",
    "                    phrasedata.append(parts[-1])\n",
    "    if flag=='train':\n",
    "        samples_per_class = 4000\n",
    "        balanced_data = []\n",
    "        labels = ['0', '1', '2', '3', '4']   \n",
    "        for label in labels:\n",
    "            class_data = [item for item in phrasedata if item[1] == label]\n",
    "            if len(class_data) >= samples_per_class:\n",
    "                class_sample = class_data[:samples_per_class]\n",
    "            else:\n",
    "                class_sample = class_data \n",
    "            balanced_data.extend(class_sample)\n",
    "            \n",
    "        random.shuffle(balanced_data)\n",
    "        \n",
    "        phraselist = balanced_data\n",
    "    elif flag=='test':\n",
    "        phraselist = phrasedata[:10000]\n",
    "    \n",
    "    #nltk.download('stopwords')\n",
    "    nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "    morestopwords = ['could', 'would', 'might', 'must', 'need', 'sha', 'wo', 'y', \"'s\", \"'d\", \"'ll\", \"'t\", \"'m\", \"'re\", \"'ve\", \"n't\"]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    #pos_list, neutral_list, neg_list = readSubjectivity(lexicon_path)\n",
    "    \n",
    "    output_lines = []\n",
    "    for item in phraselist:\n",
    "        if flag == 'train':\n",
    "            phrase, label = item\n",
    "        else:\n",
    "            phrase = item\n",
    "        tokens = nltk.word_tokenize(phrase)\n",
    "        stopwords = set(nltkstopwords+morestopwords) \n",
    "        stopwords = [word for word in stopwords if word not in negationwords]\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "        filtered_tokens = [word for word in lemmatized_tokens if word.lower() not in stopwords and not all(char in punctuation for char in word)]\n",
    "        if flag == 'train':  \n",
    "            output_lines.append(','.join(filtered_tokens) + ',' + label)\n",
    "        else:  \n",
    "            output_lines.append(','.join(filtered_tokens))\n",
    "        \n",
    "    return output_lines\n",
    "\n",
    "  \n",
    "path= \"/Users/subhiksha/Documents/NLP/NLP project/FinalProjectData/kagglemoviereviews/corpus\"\n",
    "train_data = processkaggle(path,'train')\n",
    "train_data = [entry for entry in train_data if entry and ',' in entry and entry.split(',')[0].strip()]\n",
    "\n",
    "test_data = processkaggle(path,'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7179a083-a438-4aa6-8f10-59841607adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.6969697  0.68863636 0.68888889 0.68982066 0.68931548]\n",
      "Average CV Score: 0.6907262189972471\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "train_documents = [' '.join(line.split(',')[:-1]) for line in train_data]\n",
    "train_labels = [int(line.split(',')[-1]) for line in train_data]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_vect = vectorizer.fit_transform(train_documents) \n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "cv_scores = cross_val_score(model, train_vect, train_labels, cv=5)\n",
    "\n",
    "print(\"CV Scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", np.mean(cv_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2315d2-b7c1-4fd8-8963-5b3da9f7310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1.0, 'solver': 'liblinear'}\n",
      "Best CV score: 0.6911306165979064\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train_documents = [' '.join(line.split(',')[:-1]) for line in train_data]\n",
    "train_labels = [int(line.split(',')[-1]) for line in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "train_vect = vectorizer.fit_transform(train_documents)\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-3, 3, 7),  \n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] \n",
    "}\n",
    "model = GridSearchCV(LogisticRegression(max_iter=5000), param_grid , cv=5, scoring='accuracy')\n",
    "model.fit(train_vect, train_labels)\n",
    "print(\"Best parameters:\", model.best_params_)\n",
    "print(\"Best CV score:\", model.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79a71ad-d7a1-4107-a562-0c613c1b39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# test_documents = [line for line in test_data]\n",
    "# test_documents = [' '.join(line.split(',')[:-1]) for line in test_data]\n",
    "\n",
    "# X_test_vect = vectorizer.transform(test_documents)  # Only transform test data\n",
    "# y_pred = model.predict(X_test_vect)\n",
    "# print(\"Test Predictions:\", y_pred)\n",
    "\n",
    "\n",
    "# output_csv_path = '/Users/subhiksha/Documents/NLP/final_test_predictions.csv'\n",
    "# with open(output_csv_path, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Phrase', 'Prediction']) \n",
    "#     for phrase, prediction in zip(test_documents, y_pred):\n",
    "#         writer.writerow([phrase, prediction])\n",
    "\n",
    "# print(\"Predictions have been saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4a59c-c8d7-4723-9850-df9ffdff51c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
