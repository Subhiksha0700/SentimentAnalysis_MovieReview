{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "532ca669-c29f-44b2-99f1-72d3a21804fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from sentiment_read_subjectivity import readSubjectivity\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lexicon_path = \"/Users/subhiksha/Documents/NLP/subjclueslen1-HLTEMNLP05.tff\"\n",
    "data = []\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "     \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def processkaggle(dirPath,flag):\n",
    "    os.chdir(dirPath)\n",
    "    if flag=='train':\n",
    "        filepath = './train.tsv'\n",
    "    else:\n",
    "        filepath = './test.tsv'\n",
    "    with open(filepath, 'r') as f:   \n",
    "        phrasedata = []\n",
    "        for line in f:\n",
    "            if not line.startswith('Phrase'):\n",
    "                line = line.strip()\n",
    "                parts = line.split('\\t')           \n",
    "                if flag == 'train':\n",
    "                    phrasedata.append((parts[2], parts[3])) \n",
    "                else:\n",
    "                    phrasedata.append(parts[-1])\n",
    "    if flag=='train':\n",
    "        samples_per_class = 4000\n",
    "        balanced_data = []\n",
    "        labels = ['0', '1', '2', '3', '4']   \n",
    "        for label in labels:\n",
    "            class_data = [item for item in phrasedata if item[1] == label]\n",
    "            if len(class_data) >= samples_per_class:\n",
    "                class_sample = class_data[:samples_per_class]\n",
    "            else:\n",
    "                class_sample = class_data \n",
    "            balanced_data.extend(class_sample)\n",
    "            \n",
    "        random.shuffle(balanced_data)\n",
    "        \n",
    "        phraselist = balanced_data\n",
    "    elif flag=='test':\n",
    "        phraselist = phrasedata[:10000]\n",
    "    \n",
    "    #nltk.download('stopwords')\n",
    "    nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "    morestopwords = ['could', 'would', 'might', 'must', 'need', 'sha', 'wo', 'y', \"'s\", \"'d\", \"'ll\", \"'t\", \"'m\", \"'re\", \"'ve\", \"n't\"]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    #pos_list, neutral_list, neg_list = readSubjectivity(lexicon_path)\n",
    "    \n",
    "    output_lines = []\n",
    "    for item in phraselist:\n",
    "        if flag == 'train':\n",
    "            phrase, label = item\n",
    "        else:\n",
    "            phrase = item\n",
    "        tokens = nltk.word_tokenize(phrase)\n",
    "        stopwords = set(nltkstopwords+morestopwords) \n",
    "        stopwords = [word for word in stopwords if word not in negationwords]\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "        filtered_tokens = [word for word in lemmatized_tokens if word.lower() not in stopwords and not all(char in punctuation for char in word)]\n",
    "        if flag == 'train':  \n",
    "            output_lines.append(','.join(filtered_tokens) + ',' + label)\n",
    "        else:  \n",
    "            output_lines.append(','.join(filtered_tokens))\n",
    "        \n",
    "    return output_lines\n",
    "\n",
    "  \n",
    "path= \"/Users/subhiksha/Documents/NLP/NLP project/FinalProjectData/kagglemoviereviews/corpus\"\n",
    "train_data = processkaggle(path,'train')\n",
    "test_data = processkaggle(path,'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d9c3d3c-0a7a-4e94-991d-1703cca76859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words.\n",
      "Cross-Validation Accuracy Scores: [0.63875 0.65825 0.643   0.64575 0.6515 ]\n",
      "Mean CV Accuracy: 0.64745\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    glove_model = {}\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]], dtype=np.float32)\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"Loaded {len(glove_model)} words.\")\n",
    "    return glove_model\n",
    "\n",
    "glove_path = '/Users/subhiksha/Downloads/glove.6B/glove.6B.100d.txt'\n",
    "glove_model = load_glove_embeddings(glove_path)\n",
    "\n",
    "documents = [' '.join(line.split(',')[:-1]) for line in train_data]\n",
    "labels = [int(line.split(',')[-1]) for line in train_data]\n",
    "\n",
    "def document_vector_glove(document, embeddings):\n",
    "    words = document.split()  # Split document into words\n",
    "    word_vectors = [embeddings.get(word, np.zeros(100)) for word in words]  # Handle out-of-vocabulary words\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(100)\n",
    "\n",
    "X_glove = np.array([document_vector_glove(doc, glove_model) for doc in documents])\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "classifier.fit(X_glove, labels) \n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)  \n",
    "scores = cross_val_score(classifier, X_glove, labels, cv=kf)\n",
    "\n",
    "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
    "print(\"Mean CV Accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef1977ba-460f-4bfa-9188-5a152f8684b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been saved to: /Users/subhiksha/Documents/NLP/final_test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "test_documents = [line for line in test_data]\n",
    "test_documents = [' '.join(line.split(',')[:-1]) for line in test_data]\n",
    "\n",
    "X_test = np.array([document_vector_glove(doc, glove_model) for doc in test_documents])\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "output_csv_path = '/Users/subhiksha/Documents/NLP/final_test_predictions.csv'\n",
    "with open(output_csv_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Phrase', 'Prediction']) \n",
    "    for phrase, prediction in zip(test_documents, y_pred):\n",
    "        writer.writerow([phrase, prediction])\n",
    "\n",
    "print(\"Predictions have been saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb63049-7bb3-4582-8e41-4e280e68fe1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2d5ac80-e5d7-4dbe-8b1f-9782ea313635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# import warnings\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# # Train a FastText model\n",
    "# model = fasttext.train_unsupervised('/Users/subhiksha/Documents/NLP/train_data.txt', model='skipgram')\n",
    "\n",
    "# # Function to convert document to mean of word vectors\n",
    "# def document_vector(doc):\n",
    "#     word_vectors = [model.get_word_vector(word) for word in doc]\n",
    "#     return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.get_dimension())\n",
    "\n",
    "# # Extract documents and labels\n",
    "\n",
    "# documents = [line.split(',')[:-1] for line in train_data]  \n",
    "# labels = [int(line.split(',')[-1]) for line in train_data]  \n",
    "\n",
    "# X = np.array([document_vector(doc) for doc in documents])\n",
    "# y = np.array(labels)\n",
    "\n",
    "# #classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "#                             module=\"sklearn\")\n",
    "#     classifier = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42)\n",
    "#     classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=42, shuffle=True)  \n",
    "\n",
    "# scores = cross_val_score(classifier, X, y, cv=kf)\n",
    "\n",
    "# print(\"Cross-validation scores:\", scores)\n",
    "# print(\"Average accuracy:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f197842-48a0-4318-95d8-e5ff45a95a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_glove: 20000\n",
      "Length of labels (y): 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of X_glove:\", len(X_glove))\n",
    "print(\"Length of labels (y):\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9620e-265f-4a86-9f0b-ec43c1707963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fasttext.train_unsupervised('/Users/subhiksha/Documents/NLP/train_data.txt', model='skipgram')\n",
    "\n",
    "# def document_vector(doc):\n",
    "#     word_vectors = [model.get_word_vector(word) for word in doc]\n",
    "#     return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.get_dimension())\n",
    "\n",
    "\n",
    "# X_fasttext = np.array([document_vector(doc) for doc in documents])\n",
    "# y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa771b84-7f50-4069-8736-b8d61b61511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not,mention,inappropriate,wildly,undeserved,0',\n",
       " 'terrific,special,effect,4',\n",
       " 'sensational,true-crime,4',\n",
       " 'make,movie,depth,man,lack,1',\n",
       " 'cinematic,shell,game,2']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "796be4c5-b664-434c-8f73-4f7562272f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to /Users/subhiksha/Documents/NLP/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path and name\n",
    "filename = \"/Users/subhiksha/Documents/NLP/train.csv\"  \n",
    "\n",
    "# # Open the file in write mode\n",
    "# with open(filename, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     for line in train_data:\n",
    "#         row = line.split(',')\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"Data has been written to {filename}\")\n",
    "\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Phrase', 'Prediction']) \n",
    "    for line in train_data:\n",
    "        elements = line.split(',')\n",
    "        phrase = ','.join(elements[:-1])\n",
    "        prediction = elements[-1]\n",
    "        writer.writerow([phrase, prediction])\n",
    "\n",
    "print(f\"Data has been written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6dd87-adfe-47e6-a90c-1f09e4910af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
